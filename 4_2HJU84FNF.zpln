{
  "paragraphs": [
    {
      "title": "Spark UI",
      "text": "%md\nhttp://10.4.49.41:8088/ui2/#/yarn-apps/apps",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:14+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><a href=\"http://10.4.49.41:8088/ui2/#/yarn-apps/apps\">http://10.4.49.41:8088/ui2/#/yarn-apps/apps</a></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031229_1426483413",
      "id": "paragraph_1667498596830_2120365007",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:17:14+0300",
      "dateFinished": "2022-11-08T20:17:14+0300",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:65"
    },
    {
      "text": "%spark.conf\n\nspark.executor.instances=2\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031229_90052961",
      "id": "20211125-171509_574539172",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:17:15+0300",
      "dateFinished": "2022-11-08T20:17:15+0300",
      "status": "FINISHED",
      "$$hashKey": "object:66"
    },
    {
      "title": "Как выполнять",
      "text": "%md\n.\n.\n.\nНужно скопировать себе эту тетрадку и предоставить доступ к копии на чтение, запись и запуск тетрадки пользователю admin. Параграфы с генерацией данных и созданием семплов запускать не нужно, они оставлены для ознакомления",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>.<br />\n.<br />\n.<br />\nНужно скопировать себе эту тетрадку и предоставить доступ к копии на чтение, запись и запуск тетрадки пользователю admin. Параграфы с генерацией данных и созданием семплов запускать не нужно, они оставлены для ознакомления</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031229_1620746237",
      "id": "20201127-213054_1829929461",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:17:15+0300",
      "dateFinished": "2022-11-08T20:17:15+0300",
      "status": "FINISHED",
      "$$hashKey": "object:67"
    },
    {
      "text": "print(\"https://arena-hadoop.inno.tech:18088/proxy/\" + sc.applicationId)",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "https://arena-hadoop.inno.tech:18088/proxy/application_1667741035038_0372"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031229_1300966145",
      "id": "20210412-130850_152897354",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:17:15+0300",
      "dateFinished": "2022-11-08T20:17:15+0300",
      "status": "FINISHED",
      "$$hashKey": "object:68"
    },
    {
      "title": "Генерация events таблицы (код для ознакомления, запускать не нужно)",
      "text": "import org.apache.spark.mllib.random.RandomRDDs._\nimport java.time.LocalDate\nimport java.time.format.DateTimeFormatter\n\nval dates = (0 to 14).map(LocalDate.of(2020, 11, 1).plusDays(_).format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\"))).toSeq\n\ndef generateCity(r: Double): String = if (r < 0.9) \"BIG_CITY\" else \"SMALL_CITY_\" + scala.math.round((r - 0.9) * 1000)\n\ndef generateCityUdf = udf(generateCity _)\n\nspark.sql(\"create database hw_4\")\n\nfor(i <- dates) {\n    uniformRDD(sc, 10000000L, 1)\n    .toDF(\"uid\")\n    .withColumn(\"date\", lit(i))\n    .withColumn(\"city\", generateCityUdf($\"uid\"))\n    .selectExpr(\"date\", \" sha2(cast(uid as STRING), 256) event_id\", \"city\")\n    .withColumn(\"skew_key\", when($\"city\" === \"BIG_CITY\", lit(\"big_event\")).otherwise($\"event_id\"))\n    .write.mode(\"append\")\n    .partitionBy(\"date\")\n    .saveAsTable(\"hw_4.events_full\")\n}\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031229_1699529670",
      "id": "20201127-224038_803369215",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "FINISHED",
      "$$hashKey": "object:69"
    },
    {
      "title": "Генерация events_sample",
      "text": "spark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.001)\n.repartition(2)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample\")\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 3,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_468855571",
      "id": "20201127-230139_1962818180",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "FINISHED",
      "$$hashKey": "object:70"
    },
    {
      "text": "\nspark.table(\"hw_4.sample\")\n.limit(100)\n.coalesce(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_small\")",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 3,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_548535823",
      "id": "20201128-000812_530567540",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "FINISHED",
      "$$hashKey": "object:71"
    },
    {
      "text": "\n\nspark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.003)\n.repartition(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_big\")",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 3,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_2127551641",
      "id": "20201128-091248_492627774",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "FINISHED",
      "$$hashKey": "object:72"
    },
    {
      "text": "\n\nspark.table(\"hw_4.events_full\")\n.select(\"event_id\")\n.sample(0.015)\n.repartition(1)\n.write.mode(\"overwrite\")\n.saveAsTable(\"hw_4.sample_very_big\")",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 3,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_96001976",
      "id": "20201128-093907_1614062530",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "FINISHED",
      "$$hashKey": "object:73"
    },
    {
      "title": "Задание 1",
      "text": "%md\n\n\n\nДля упражнений сгрененирован большой набор синтетических данных в таблице hw2.events_full. Из этого набора данных созданы маленькие (относительно исходного набора) таблицы разного размера kotelnikov.sample_[small, big, very_big]. \n\nОтветить на вопросы:\n * какова структура таблиц\n * сколько в них записей \n * сколько места занимают данные\n ",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Для упражнений сгрененирован большой набор синтетических данных в таблице hw2.events_full. Из этого набора данных созданы маленькие (относительно исходного набора) таблицы разного размера kotelnikov.sample_[small, big, very_big].</p>\n<p>Ответить на вопросы:</p>\n<ul>\n<li>какова структура таблиц</li>\n<li>сколько в них записей</li>\n<li>сколько места занимают данные</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_28171443",
      "id": "20201128-094640_2955666",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:17:15+0300",
      "dateFinished": "2022-11-08T20:17:15+0300",
      "status": "FINISHED",
      "$$hashKey": "object:74"
    },
    {
      "title": "1.1 Структура таблиц",
      "text": "%pyspark\n\nspark.read.parquet(\"/apps/hive/warehouse/hw_4.db/events_full\").printSchema()\n\nspark.read.parquet(\"/apps/hive/warehouse/hw_4.db/events_full/date=2020-11-02\").printSchema()\n\nspark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample\").printSchema()\n\nspark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample_big\").printSchema()\n\nspark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample_very_big\").printSchema()",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:15+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- event_id: string (nullable = true)\n |-- city: string (nullable = true)\n |-- skew_key: string (nullable = true)\n |-- date: date (nullable = true)\n\nroot\n |-- event_id: string (nullable = true)\n |-- city: string (nullable = true)\n |-- skew_key: string (nullable = true)\n\nroot\n |-- event_id: string (nullable = true)\n\nroot\n |-- event_id: string (nullable = true)\n\nroot\n |-- event_id: string (nullable = true)\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=20",
              "$$hashKey": "object:943"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=21",
              "$$hashKey": "object:944"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=22",
              "$$hashKey": "object:945"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=23",
              "$$hashKey": "object:946"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=24",
              "$$hashKey": "object:947"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667851200753_274302646",
      "id": "paragraph_1667851200753_274302646",
      "dateCreated": "2022-11-07T23:00:00+0300",
      "dateStarted": "2022-11-08T20:17:15+0300",
      "dateFinished": "2022-11-08T20:17:16+0300",
      "status": "FINISHED",
      "$$hashKey": "object:75"
    },
    {
      "title": "1.2 Количество записей",
      "text": "%pyspark\n\nprint(spark.read.parquet(\"/apps/hive/warehouse/hw_4.db/events_full\").count())\n\nprint(spark.read.parquet(\"/apps/hive/warehouse/hw_4.db/events_full/date=2020-11-02\").count())\n\nprint(spark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample_small\").count())\n\nprint(spark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample_big\").count())\n\nspark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample_very_big\").count()",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:16+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "140000000\n10000000\n100\n139853\n2102869\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=25",
              "$$hashKey": "object:967"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=26",
              "$$hashKey": "object:968"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=27",
              "$$hashKey": "object:969"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=28",
              "$$hashKey": "object:970"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=29",
              "$$hashKey": "object:971"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=30",
              "$$hashKey": "object:972"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=31",
              "$$hashKey": "object:973"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=32",
              "$$hashKey": "object:974"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=33",
              "$$hashKey": "object:975"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=34",
              "$$hashKey": "object:976"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667852751921_2043941637",
      "id": "paragraph_1667852751921_2043941637",
      "dateCreated": "2022-11-07T23:25:51+0300",
      "dateStarted": "2022-11-08T20:17:16+0300",
      "dateFinished": "2022-11-08T20:17:18+0300",
      "status": "FINISHED",
      "$$hashKey": "object:76"
    },
    {
      "title": "1.3 Объем",
      "text": "%sh\n\nhdfs dfs -du -h -s /apps/hive/warehouse/hw_4.db/events_full\n\n\nhdfs dfs -du -h -s /apps/hive/warehouse/hw_4.db/sample_small\n\nhdfs dfs -du -h -s /apps/hive/warehouse/hw_4.db/sample_big\n\nhdfs dfs -du -h -s /apps/hive/warehouse/hw_4.db/sample_very_big",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:18+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "9.3 G  27.9 G  /apps/hive/warehouse/hw_4.db/events_full\n7.0 K  21.1 K  /apps/hive/warehouse/hw_4.db/sample_small\n8.5 M  25.5 M  /apps/hive/warehouse/hw_4.db/sample_big\n127.7 M  383.2 M  /apps/hive/warehouse/hw_4.db/sample_very_big\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836287386_1422277888",
      "id": "paragraph_1667836287386_1422277888",
      "dateCreated": "2022-11-07T18:51:27+0300",
      "dateStarted": "2022-11-08T20:17:18+0300",
      "dateFinished": "2022-11-08T20:17:25+0300",
      "status": "FINISHED",
      "$$hashKey": "object:77"
    },
    {
      "text": "%sh\n\nhdfs dfs -count -h /apps/hive/warehouse/hw_4.db/sample_small\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:25+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "           1            2              7.0 K /apps/hive/warehouse/hw_4.db/sample_small\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667853106819_1526076165",
      "id": "paragraph_1667853106819_1526076165",
      "dateCreated": "2022-11-07T23:31:46+0300",
      "dateStarted": "2022-11-08T20:17:25+0300",
      "dateFinished": "2022-11-08T20:17:27+0300",
      "status": "FINISHED",
      "$$hashKey": "object:78"
    },
    {
      "title": "Задание 2",
      "text": "%md\n.\n.\n.\n\nПолучить планы запросов для джойна большой таблицы hw2.events_full с каждой из таблиц hw2.sample, hw2.sample_big, hw2.sample_very_big по полю event_id. В каких случаях используется BroadcastHashJoin? \n\nBroadcastHashJoin автоматически выполняется для джойна с таблицами, размером меньше параметра spark.sql.autoBroadcastJoinThreshold. Узнать его значение можно командой spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\").",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:27+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>.<br />\n.<br />\n.</p>\n<p>Получить планы запросов для джойна большой таблицы hw2.events_full с каждой из таблиц hw2.sample, hw2.sample_big, hw2.sample_very_big по полю event_id. В каких случаях используется BroadcastHashJoin?</p>\n<p>BroadcastHashJoin автоматически выполняется для джойна с таблицами, размером меньше параметра spark.sql.autoBroadcastJoinThreshold. Узнать его значение можно командой spark.conf.get(&ldquo;spark.sql.autoBroadcastJoinThreshold&rdquo;).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_778787598",
      "id": "20201128-132950_831220047",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:17:27+0300",
      "dateFinished": "2022-11-08T20:17:27+0300",
      "status": "FINISHED",
      "$$hashKey": "object:79"
    },
    {
      "title": "2. Планы запросов",
      "text": "%pyspark \n\nhw_f = spark.read.parquet(\"/apps/hive/warehouse/hw_4.db/events_full\")\nhw_s = spark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample\")\nhw_b = spark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample_big\")\nhw_vb = spark.read.parquet(\"/apps/hive/warehouse/hw_4.db/sample_very_big\")\n\ndfs = [hw_s,hw_b,hw_vb]\n\nfor x in dfs:\n    hw_f.join(x,hw_f.event_id==x.event_id).explain(True)\n    print(\"\\n-----------------\\n\")\n    ",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:27+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "== Parsed Logical Plan ==\nJoin Inner, (event_id#714 = event_id#722)\n:- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Relation[event_id#722] parquet\n\n== Analyzed Logical Plan ==\nevent_id: string, city: string, skew_key: string, date: date, event_id: string\nJoin Inner, (event_id#714 = event_id#722)\n:- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Relation[event_id#722] parquet\n\n== Optimized Logical Plan ==\nJoin Inner, (event_id#714 = event_id#722)\n:- Filter isnotnull(event_id#714)\n:  +- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Filter isnotnull(event_id#722)\n   +- Relation[event_id#722] parquet\n\n== Physical Plan ==\n*(2) BroadcastHashJoin [event_id#714], [event_id#722], Inner, BuildRight\n:- *(2) Project [event_id#714, city#715, skew_key#716, date#717]\n:  +- *(2) Filter isnotnull(event_id#714)\n:     +- *(2) FileScan parquet [event_id#714,city#715,skew_key#716,date#717] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events_full], PartitionCount: 14, PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n   +- *(1) Project [event_id#722]\n      +- *(1) Filter isnotnull(event_id#722)\n         +- *(1) FileScan parquet [event_id#722] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/sample], PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string>\n\n-----------------\n\n== Parsed Logical Plan ==\nJoin Inner, (event_id#714 = event_id#724)\n:- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Relation[event_id#724] parquet\n\n== Analyzed Logical Plan ==\nevent_id: string, city: string, skew_key: string, date: date, event_id: string\nJoin Inner, (event_id#714 = event_id#724)\n:- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Relation[event_id#724] parquet\n\n== Optimized Logical Plan ==\nJoin Inner, (event_id#714 = event_id#724)\n:- Filter isnotnull(event_id#714)\n:  +- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Filter isnotnull(event_id#724)\n   +- Relation[event_id#724] parquet\n\n== Physical Plan ==\n*(2) BroadcastHashJoin [event_id#714], [event_id#724], Inner, BuildRight\n:- *(2) Project [event_id#714, city#715, skew_key#716, date#717]\n:  +- *(2) Filter isnotnull(event_id#714)\n:     +- *(2) FileScan parquet [event_id#714,city#715,skew_key#716,date#717] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events_full], PartitionCount: 14, PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n   +- *(1) Project [event_id#724]\n      +- *(1) Filter isnotnull(event_id#724)\n         +- *(1) FileScan parquet [event_id#724] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/sample_big], PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string>\n\n-----------------\n\n== Parsed Logical Plan ==\nJoin Inner, (event_id#714 = event_id#726)\n:- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Relation[event_id#726] parquet\n\n== Analyzed Logical Plan ==\nevent_id: string, city: string, skew_key: string, date: date, event_id: string\nJoin Inner, (event_id#714 = event_id#726)\n:- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Relation[event_id#726] parquet\n\n== Optimized Logical Plan ==\nJoin Inner, (event_id#714 = event_id#726)\n:- Filter isnotnull(event_id#714)\n:  +- Relation[event_id#714,city#715,skew_key#716,date#717] parquet\n+- Filter isnotnull(event_id#726)\n   +- Relation[event_id#726] parquet\n\n== Physical Plan ==\n*(5) SortMergeJoin [event_id#714], [event_id#726], Inner\n:- *(2) Sort [event_id#714 ASC NULLS FIRST], false, 0\n:  +- Exchange hashpartitioning(event_id#714, 200)\n:     +- *(1) Project [event_id#714, city#715, skew_key#716, date#717]\n:        +- *(1) Filter isnotnull(event_id#714)\n:           +- *(1) FileScan parquet [event_id#714,city#715,skew_key#716,date#717] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events_full], PartitionCount: 14, PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n+- *(4) Sort [event_id#726 ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(event_id#726, 200)\n      +- *(3) Project [event_id#726]\n         +- *(3) Filter isnotnull(event_id#726)\n            +- *(3) FileScan parquet [event_id#726] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/sample_very_..., PartitionFilters: [], PushedFilters: [IsNotNull(event_id)], ReadSchema: struct<event_id:string>\n\n-----------------\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=35",
              "$$hashKey": "object:1033"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=36",
              "$$hashKey": "object:1034"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=37",
              "$$hashKey": "object:1035"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=38",
              "$$hashKey": "object:1036"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667853645832_2059687122",
      "id": "paragraph_1667853645832_2059687122",
      "dateCreated": "2022-11-07T23:40:45+0300",
      "dateStarted": "2022-11-08T20:17:27+0300",
      "dateFinished": "2022-11-08T20:17:28+0300",
      "status": "FINISHED",
      "$$hashKey": "object:80"
    },
    {
      "text": "%md\n\nBroadcastHashJoin использовался для джойна исходной таблицы с sample и sample_big, а для джойна с sample_very_big использовался SortMergeJoin, так как эта таблица занимает больше места, чем обозначенный порог для BroadcastHashJoin.\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:28+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>BroadcastHashJoin использовался для джойна исходной таблицы с sample и sample_big, а для джойна с sample_very_big использовался SortMergeJoin, так как эта таблица занимает больше места, чем обозначенный порог для BroadcastHashJoin.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667909628429_472663017",
      "id": "paragraph_1667909628429_472663017",
      "dateCreated": "2022-11-08T15:13:48+0300",
      "dateStarted": "2022-11-08T20:17:28+0300",
      "dateFinished": "2022-11-08T20:17:28+0300",
      "status": "FINISHED",
      "$$hashKey": "object:81"
    },
    {
      "text": "%sh\n\nhdfs dfs -du -s /apps/hive/warehouse/hw_4.db/sample_very_big\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:28+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "133939664  401818992  /apps/hive/warehouse/hw_4.db/sample_very_big\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667910224869_2059843964",
      "id": "paragraph_1667910224869_2059843964",
      "dateCreated": "2022-11-08T15:23:44+0300",
      "dateStarted": "2022-11-08T20:17:28+0300",
      "dateFinished": "2022-11-08T20:17:29+0300",
      "status": "FINISHED",
      "$$hashKey": "object:82"
    },
    {
      "text": "%pyspark\n\nspark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:30+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "u'10485760'\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667853580887_988254839",
      "id": "paragraph_1667853580887_988254839",
      "dateCreated": "2022-11-07T23:39:40+0300",
      "dateStarted": "2022-11-08T20:17:30+0300",
      "dateFinished": "2022-11-08T20:17:30+0300",
      "status": "FINISHED",
      "$$hashKey": "object:83"
    },
    {
      "title": "Задание 3",
      "text": "%md\n\nВыполнить джойны с таблицами  hw2.sample,  hw2.sample_big в отдельных параграфах, чтобы узнать время выполнения запросов (например, вызвать .count() для результатов запросов). Время выполнения параграфа считается автоматически и указывается в нижней части по завершении\n\nЗайти в spark ui (ссылку сгенерировать в следующем папраграфе). Сколько tasks создано на каждую операцию? Почему именно столько? Каков DAG вычислений?  ",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:30+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Выполнить джойны с таблицами  hw2.sample,  hw2.sample_big в отдельных параграфах, чтобы узнать время выполнения запросов (например, вызвать .count() для результатов запросов). Время выполнения параграфа считается автоматически и указывается в нижней части по завершении</p>\n<p>Зайти в spark ui (ссылку сгенерировать в следующем папраграфе). Сколько tasks создано на каждую операцию? Почему именно столько? Каков DAG вычислений?</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_1740432675",
      "id": "20201128-140231_1065047171",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:17:30+0300",
      "dateFinished": "2022-11-08T20:17:30+0300",
      "status": "FINISHED",
      "$$hashKey": "object:84"
    },
    {
      "text": "%pyspark\n\nhw_f.join(hw_s,\"event_id\").count()\n\n# запрос выполнялся 53 секунды",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:17:30+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "140323\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=39",
              "$$hashKey": "object:1090"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=40",
              "$$hashKey": "object:1091"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667910405886_1933708267",
      "id": "paragraph_1667910405886_1933708267",
      "dateCreated": "2022-11-08T15:26:45+0300",
      "dateStarted": "2022-11-08T20:17:30+0300",
      "dateFinished": "2022-11-08T20:18:12+0300",
      "status": "FINISHED",
      "$$hashKey": "object:85"
    },
    {
      "text": "%pyspark\n\nhw_f.join(hw_b,\"event_id\").count()\n\n# запрос выполнялся 39 секунд",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:18:12+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "139853\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=41",
              "$$hashKey": "object:1105"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=42",
              "$$hashKey": "object:1106"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667910654257_1714271713",
      "id": "paragraph_1667910654257_1714271713",
      "dateCreated": "2022-11-08T15:30:54+0300",
      "dateStarted": "2022-11-08T20:18:12+0300",
      "dateFinished": "2022-11-08T20:18:54+0300",
      "status": "FINISHED",
      "$$hashKey": "object:86"
    },
    {
      "title": "Генерация ссылки на  spark UI",
      "text": "println(\"http://10.4.49.41:8088/proxy/\" + sc.applicationId + \"/jobs/\")",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:18:54+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "http://10.4.49.41:8088/proxy/application_1667741035038_0372/jobs/\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_2071850807",
      "id": "20201128-150602_756898802",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:18:54+0300",
      "dateFinished": "2022-11-08T20:18:54+0300",
      "status": "FINISHED",
      "$$hashKey": "object:87"
    },
    {
      "title": "3.1 Первый запрос",
      "text": "%md\n\nНа операцию было создано 76 tasks. Количество тасок зависит от кол-ва и размеров блоков. Также может зависить от кол-ва партиций, экзекьюторов и используемых ядер. ",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:18:54+0300",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>На операцию было создано 76 tasks. Количество тасок зависит от кол-ва и размеров блоков. Также может зависить от кол-ва партиций, экзекьюторов и используемых ядер.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667911965984_1285868121",
      "id": "paragraph_1667911965984_1285868121",
      "dateCreated": "2022-11-08T15:52:45+0300",
      "dateStarted": "2022-11-08T20:18:54+0300",
      "dateFinished": "2022-11-08T20:18:54+0300",
      "status": "FINISHED",
      "$$hashKey": "object:88"
    },
    {
      "title": "3.2 Второй запрос",
      "text": "%md\n\nНа операцию было создано 68 tasks. Размер входных данных 7.6 gb. На каждую таску выделилось по 144,5 mb. Соответственно 144,5 * 68 ≈ 7.6 gb.\n\nDAG выглядит следующим образом:\n![alt text](https://i.ibb.co/R02XH8w/dag2.png)",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:18:54+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>На операцию было создано 68 tasks. Размер входных данных 7.6 gb. На каждую таску выделилось по 144,5 mb. Соответственно 144,5 * 68 ≈ 7.6 gb.</p>\n<p>DAG выглядит следующим образом:<br />\n<img src=\"https://i.ibb.co/R02XH8w/dag2.png\" alt=\"alt text\" /></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667911763860_1666249543",
      "id": "paragraph_1667911763860_1666249543",
      "dateCreated": "2022-11-08T15:49:23+0300",
      "dateStarted": "2022-11-08T20:18:54+0300",
      "dateFinished": "2022-11-08T20:18:54+0300",
      "status": "FINISHED",
      "$$hashKey": "object:89"
    },
    {
      "title": "Насильный broadcast",
      "text": "%md\n\nОптимизировать джойн с таблицами hw2.sample_big, hw2.sample_very_big с помощью broadcast(df). Выполнить запрос, посмотреть в UI, как поменялся план запроса, DAG, количество тасков. Второй запрос не выполнится ",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:18:55+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Оптимизировать джойн с таблицами hw2.sample_big, hw2.sample_very_big с помощью broadcast(df). Выполнить запрос, посмотреть в UI, как поменялся план запроса, DAG, количество тасков. Второй запрос не выполнится</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_901531372",
      "id": "20201128-140749_375295552",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:18:55+0300",
      "dateFinished": "2022-11-08T20:18:55+0300",
      "status": "FINISHED",
      "$$hashKey": "object:90"
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.functions import broadcast\n\nhw_f.join(broadcast(hw_b),\"event_id\")",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:18:55+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DataFrame[event_id: string, city: string, skew_key: string, date: date]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667913933991_595357074",
      "id": "paragraph_1667913933991_595357074",
      "dateCreated": "2022-11-08T16:25:33+0300",
      "dateStarted": "2022-11-08T20:18:55+0300",
      "dateFinished": "2022-11-08T20:18:55+0300",
      "status": "FINISHED",
      "$$hashKey": "object:91"
    },
    {
      "text": "%pyspark\n\nhw_f.join(broadcast(hw_b),\"event_id\").count()",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:18:55+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "139853\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=43",
              "$$hashKey": "object:1165"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=44",
              "$$hashKey": "object:1166"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667916107973_214420093",
      "id": "paragraph_1667916107973_214420093",
      "dateCreated": "2022-11-08T17:01:47+0300",
      "dateStarted": "2022-11-08T20:18:55+0300",
      "dateFinished": "2022-11-08T20:19:36+0300",
      "status": "FINISHED",
      "$$hashKey": "object:92"
    },
    {
      "text": "%md\n\nВызывая, например, функцию count(), к джойну с и без broadcast, я не вижу разницы в spark ui...",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:19:36+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Вызывая, например, функцию count(), к джойну с и без broadcast, я не вижу разницы в spark ui&hellip;</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667916681516_2084479616",
      "id": "paragraph_1667916681516_2084479616",
      "dateCreated": "2022-11-08T17:11:21+0300",
      "dateStarted": "2022-11-08T20:19:36+0300",
      "dateFinished": "2022-11-08T20:19:36+0300",
      "status": "FINISHED",
      "$$hashKey": "object:93"
    },
    {
      "text": "%pyspark\n\nhw_f.join(broadcast(hw_vb),\"event_id\").show()",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:19:36+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o504.showString.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:206)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceBatches(ColumnarBatchScan.scala:138)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:78)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 87.0 failed 4 times, most recent failure: Lost task 0.3 in stage 87.0 (TID 3326, ca-spark-d-03.innoca.local, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e04_1667741035038_0372_01_000006 on host: ca-spark-d-03.innoca.local. Exit status: 143. Diagnostics: [2022-11-08 20:20:00.569]Container killed on request. Exit code is 143\n[2022-11-08 20:20:00.569]Container exited with a non-zero exit code 143. \n[2022-11-08 20:20:00.570]Killed by external signal\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:304)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:97)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o504.showString.\\n', JavaObject id=o505), <traceback object at 0x7fb893016cf8>)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=45",
              "$$hashKey": "object:1189"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667915135661_1487759618",
      "id": "paragraph_1667915135661_1487759618",
      "dateCreated": "2022-11-08T16:45:35+0300",
      "dateStarted": "2022-11-08T20:19:36+0300",
      "dateFinished": "2022-11-08T20:20:00+0300",
      "status": "ERROR",
      "$$hashKey": "object:94"
    },
    {
      "title": "Отключение auto broadcast",
      "text": "%md\n\n\n\nОтключить автоматический броадкаст командой spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\"). Сделать джойн с семплом hw_4.sample, сравнить время выполнения запроса.\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Отключить автоматический броадкаст командой spark.conf.set(&ldquo;spark.sql.autoBroadcastJoinThreshold&rdquo;, &ldquo;-1&rdquo;). Сделать джойн с семплом hw_4.sample, сравнить время выполнения запроса.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_702480022",
      "id": "20201128-092252_410955057",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:95"
    },
    {
      "text": "%pyspark\n\n\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n\nhw_f.join(hw_s,\"event_id\").count()",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T17:18:58+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "140323\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4043/jobs/job?id=4",
              "$$hashKey": "object:1210"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667916977156_1668984147",
      "id": "paragraph_1667916977156_1668984147",
      "dateCreated": "2022-11-08T17:16:17+0300",
      "dateStarted": "2022-11-08T17:18:58+0300",
      "dateFinished": "2022-11-08T17:22:00+0300",
      "status": "FINISHED",
      "$$hashKey": "object:96"
    },
    {
      "text": "%md\n\nЗапрос с отключенным автоматическим бродкастом выполняется в разы дольше и происходит spill.",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T17:25:57+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Запрос с отключенным автоматическим бродкастом выполняется в разы дольше и происходит spill.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667917353834_504033593",
      "id": "paragraph_1667917353834_504033593",
      "dateCreated": "2022-11-08T17:22:33+0300",
      "dateStarted": "2022-11-08T17:25:57+0300",
      "dateFinished": "2022-11-08T17:25:57+0300",
      "status": "FINISHED",
      "$$hashKey": "object:97"
    },
    {
      "title": "Вернуть настройку к исходной",
      "text": "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"26214400\")",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_768782301",
      "id": "20201127-230625_1272901030",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:98"
    },
    {
      "text": "spark.sql(\"clear cache\")",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res77: org.apache.spark.sql.DataFrame = []\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_267776483",
      "id": "20201128-155645_947820002",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:99"
    },
    {
      "title": "Задание 4",
      "text": "%md\n \n \n \n\nВ процессе обработки данных может возникнуть перекос объёма партиций по количеству данных (data skew). В таком случае время выполнения запроса может существенно увеличиться, так как данные распределятся по исполнителям неравномерно. В следующем параграфе происходит инициализация датафрейма, этот параграф нужно выполнить, изменять код нельзя. В задании нужно работать с инициализированным датафреймом.\n\nДатафрейм разделен на 30 партиций по ключу city, который имеет сильно  неравномерное распределение.",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>В процессе обработки данных может возникнуть перекос объёма партиций по количеству данных (data skew). В таком случае время выполнения запроса может существенно увеличиться, так как данные распределятся по исполнителям неравномерно. В следующем параграфе происходит инициализация датафрейма, этот параграф нужно выполнить, изменять код нельзя. В задании нужно работать с инициализированным датафреймом.</p>\n<p>Датафрейм разделен на 30 партиций по ключу city, который имеет сильно неравномерное распределение.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_838078408",
      "id": "20201128-163357_1545019956",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:100"
    },
    {
      "title": "нужно выполнить, изменять код нельзя",
      "text": "%pyspark \nfrom pyspark.sql.functions import col\n\nskew_df = spark.table(\"hw_4.events_full\")\\\n.where(\"date = '2020-11-02'\")\\\n.repartition(30, col(\"city\"))\\\n.cache()\n\nskew_df.count()",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:03:46+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "10000000\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=1",
              "$$hashKey": "object:1258"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_1866331556",
      "id": "20201128-162744_575252973",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "dateStarted": "2022-11-08T20:03:46+0300",
      "dateFinished": "2022-11-08T20:03:47+0300",
      "status": "FINISHED",
      "$$hashKey": "object:101"
    },
    {
      "text": "%pyspark\nsc.applicationId",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "u'application_1637409254228_0246'\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_851390068",
      "id": "20211125-203645_140241983",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:102"
    },
    {
      "title": "4.1. Наблюдение проблемы",
      "text": "%md\n.\n.\n.\n\nПосчитать количество event_count различных событий event_id , содержащихся в skew_df с группировкой по городам. Результат упорядочить по event_count.\n\nВ spark ui в разделе jobs выбрать последнюю, в ней зайти в stage, состоящую из 30 тасков (из такого количества партиций состоит skew_df). На странице стейджа нажать кнопку Event Timeline и увидеть время выполнения тасков по экзекьюторам. Одному из них выпала партиция с существенно большим количеством данных. Остальные экзекьюторы в это время бездействуют -- это и является проблемой, которую предлагается решить далее.",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<p>.\n<br  />.\n<br  />.</p>\n<p>Посчитать количество event_count различных событий event_id , содержащихся в skew_df с группировкой по городам. Результат упорядочить по event_count.</p>\n<p>В spark ui в разделе jobs выбрать последнюю, в ней зайти в stage, состоящую из 30 тасков (из такого количества партиций состоит skew_df). На странице стейджа нажать кнопку Event Timeline и увидеть время выполнения тасков по экзекьюторам. Одному из них выпала партиция с существенно большим количеством данных. Остальные экзекьюторы в это время бездействуют &ndash; это и является проблемой, которую предлагается решить далее.</p>\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_11599928",
      "id": "20201128-164139_1371291032",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:103"
    },
    {
      "text": "%pyspark\nimport pyspark.sql.functions as f\n\n#skew_df.groupBy(\"city\").count().show()\nskew_df.groupBy(\"city\").agg(f.count(\"event_id\").alias(\"event_count\")).orderBy(\"event_count\",ascending=False).show(40)\n\n#skew_df.show()",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:04:56+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------------+-----------+\n|         city|event_count|\n+-------------+-----------+\n|     BIG_CITY|    8998520|\n|SMALL_CITY_45|      10260|\n|SMALL_CITY_15|      10238|\n| SMALL_CITY_6|      10213|\n| SMALL_CITY_4|      10189|\n|SMALL_CITY_23|      10185|\n|SMALL_CITY_13|      10161|\n|SMALL_CITY_90|      10155|\n|SMALL_CITY_79|      10154|\n|SMALL_CITY_87|      10147|\n|SMALL_CITY_74|      10146|\n|SMALL_CITY_58|      10142|\n|SMALL_CITY_14|      10126|\n|SMALL_CITY_69|      10115|\n|SMALL_CITY_60|      10109|\n|SMALL_CITY_42|      10105|\n|SMALL_CITY_70|      10099|\n|SMALL_CITY_19|      10097|\n| SMALL_CITY_8|      10094|\n|SMALL_CITY_72|      10091|\n|SMALL_CITY_28|      10084|\n|SMALL_CITY_33|      10084|\n|SMALL_CITY_49|      10080|\n|SMALL_CITY_30|      10080|\n|SMALL_CITY_10|      10080|\n|SMALL_CITY_25|      10077|\n| SMALL_CITY_9|      10074|\n|SMALL_CITY_52|      10068|\n|SMALL_CITY_75|      10067|\n|SMALL_CITY_43|      10064|\n|SMALL_CITY_11|      10063|\n|SMALL_CITY_41|      10063|\n|SMALL_CITY_51|      10058|\n|SMALL_CITY_48|      10057|\n|SMALL_CITY_34|      10053|\n| SMALL_CITY_3|      10052|\n|SMALL_CITY_27|      10051|\n|SMALL_CITY_57|      10042|\n|SMALL_CITY_82|      10041|\n|SMALL_CITY_86|      10040|\n+-------------+-----------+\nonly showing top 40 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=2",
              "$$hashKey": "object:1288"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667917455557_719819082",
      "id": "paragraph_1667917455557_719819082",
      "dateCreated": "2022-11-08T17:24:15+0300",
      "dateStarted": "2022-11-08T20:04:56+0300",
      "dateFinished": "2022-11-08T20:04:59+0300",
      "status": "FINISHED",
      "$$hashKey": "object:104"
    },
    {
      "text": "%md\n\n![alt text](https://i.ibb.co/mBg6LcL/Screenshot-4.png)\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T17:45:03+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><img src=\"https://i.ibb.co/mBg6LcL/Screenshot-4.png\" alt=\"alt text\" /></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667918653246_1313957014",
      "id": "paragraph_1667918653246_1313957014",
      "dateCreated": "2022-11-08T17:44:13+0300",
      "dateStarted": "2022-11-08T17:45:03+0300",
      "dateFinished": "2022-11-08T17:45:03+0300",
      "status": "FINISHED",
      "$$hashKey": "object:105"
    },
    {
      "title": "4.2. repartition",
      "text": "%md\n.\n.\n.\n\nодин из способов решения проблемы агрегации по неравномерно распределенному ключу является предварительное перемешивание данных. Его можно сделать с помощью метода repartition(p_num), где p_num -- количество партиций, на которые будет перемешан исходный датафрейм",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>.<br/>.<br/>.</p>\n<p>один из способов решения проблемы агрегации по неравномерно распределенному ключу является предварительное перемешивание данных. Его можно сделать с помощью метода repartition(p_num), где p_num &ndash; количество партиций, на которые будет перемешан исходный датафрейм</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_1509201091",
      "id": "20201128-164814_1641460265",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:106"
    },
    {
      "text": "%pyspark\n\nskew_df2 = skew_df.repartition(40)\n\nskew_df2.groupBy(\"city\").agg(f.count(\"event_id\").alias(\"event_count\")).orderBy(\"event_count\").show()\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:16:48+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o629.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(city#1, 200)\n+- *(1) HashAggregate(keys=[city#1], functions=[partial_count(event_id#0)], output=[city#1, count#825L])\n   +- Exchange RoundRobinPartitioning(30)\n      +- InMemoryTableScan [event_id#0, city#1]\n            +- InMemoryRelation [event_id#0, city#1, skew_key#2, date#3], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n                  +- Exchange hashpartitioning(city#1, 30)\n                     +- *(1) FileScan parquet hw_4.events_full[event_id#0,city#1,skew_key#2,date#3] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events..., PartitionCount: 1, PartitionFilters: [isnotnull(date#3), (date#3 = 2020-11-02)], PushedFilters: [], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:150)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor81.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange RoundRobinPartitioning(30)\n+- InMemoryTableScan [event_id#0, city#1]\n      +- InMemoryRelation [event_id#0, city#1, skew_key#2, date#3], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n            +- Exchange hashpartitioning(city#1, 30)\n               +- *(1) FileScan parquet hw_4.events_full[event_id#0,city#1,skew_key#2,date#3] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[hdfs://ca-spark-n-01.innoca.local:8020/apps/hive/warehouse/hw_4.db/events..., PartitionCount: 1, PartitionFilters: [isnotnull(date#3), (date#3 = 2020-11-02)], PushedFilters: [], ReadSchema: struct<event_id:string,city:string,skew_key:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:150)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 36 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.needToCopyObjectsBeforeShuffle(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:303)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 56 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o629.showString.\\n', JavaObject id=o630), <traceback object at 0x7f611dd98710>)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667918122115_58607911",
      "id": "paragraph_1667918122115_58607911",
      "dateCreated": "2022-11-08T17:35:22+0300",
      "dateStarted": "2022-11-08T19:21:55+0300",
      "dateFinished": "2022-11-08T19:21:55+0300",
      "status": "ERROR",
      "$$hashKey": "object:107"
    },
    {
      "title": "4.3. Key Salting",
      "text": "%md\n.\n.\n.\nДругой способ исправить неравномерность по ключу -- создание синтетического ключа с равномерным распределением. В нашем случае неравномерность исходит от единственного значения city='BIG_CITY', которое часто повторяется в данных и при группировке попадает к одному экзекьютору. В таком случае лучше провести группировку в два этапа по синтетическому ключу CITY_SALT, который принимает значение BIG_CITY_rand (rand -- случайное целое число) для популярного значения BIG_CITY и CITY для остальных значений. На втором этапе восстанавливаем значения CITY и проводим повторную агрегацию, которая не занимает времени, потому что проводится по существенно меньшего размера данным. \n\nТакая же техника применима и к джойнам по неравномерному ключу, см, например https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8\n\nЧто нужно реализовать:\n* добавить синтетический ключ\n* группировка по синтетическому ключу\n* восстановление исходного значения\n* группировка по исходной колонке ",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>.<br/>.<br/>.<br/>Другой способ исправить неравномерность по ключу &ndash; создание синтетического ключа с равномерным распределением. В нашем случае неравномерность исходит от единственного значения city=&lsquo;BIG_CITY&rsquo;, которое часто повторяется в данных и при группировке попадает к одному экзекьютору. В таком случае лучше провести группировку в два этапа по синтетическому ключу CITY_SALT, который принимает значение BIG_CITY_rand (rand &ndash; случайное целое число) для популярного значения BIG_CITY и CITY для остальных значений. На втором этапе восстанавливаем значения CITY и проводим повторную агрегацию, которая не занимает времени, потому что проводится по существенно меньшего размера данным. </p>\n<p>Такая же техника применима и к джойнам по неравномерному ключу, см, например <a href=\"https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8\">https://itnext.io/handling-data-skew-in-apache-spark-9f56343e58e8</a></p>\n<p>Что нужно реализовать:<br/>* добавить синтетический ключ<br/>* группировка по синтетическому ключу<br/>* восстановление исходного значения<br/>* группировка по исходной колонке</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_1456843645",
      "id": "20201128-173534_1924644474",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:108"
    },
    {
      "text": "%pyspark\n\nsalt = f.expr(\"\"\"pmod(round(rand() * 100, 0), 30)\"\"\").cast(\"integer\")\n#salt = f.expr(\"\"\"pmod(round(rand() * 100, 0), 30)\"\"\").cast(\"integer\")\n\n#concat($\"state_id\", lit(\"_\"), $\"salt\"))\n\nsalted = skew_df.withColumn(\"salt\", salt)\n\nsalted.show()\n\nsalted_agg_df = salted.groupBy(f.col(\"city\"), f.col(\"salt\")).agg(f.count(\"*\").alias(\"count\")) \\\n    .orderBy(f.col(\"count\").asc())\n    \nsalted_agg_df.show(20, False)\n\nagg_df = salted_agg_df \\\n    .groupBy(f.col(\"city\")).agg(f.sum(\"count\").alias(\"final_sum\")) \\\n    .orderBy(f.col(\"final_sum\").desc())\n    \nagg_df.show(20, False)   \n#salted.select(f.col(\"type\"), f.col(\"ident\"), f.col(\"salt\")).sample(0.1).show(20, False)",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:05:02+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+-------------+--------------------+----------+----+\n|            event_id|         city|            skew_key|      date|salt|\n+--------------------+-------------+--------------------+----------+----+\n|f16f6df32cfba3822...|SMALL_CITY_40|f16f6df32cfba3822...|2020-11-02|   1|\n|7986b815b5562a99f...|SMALL_CITY_66|7986b815b5562a99f...|2020-11-02|  23|\n|8b3a9c0c0df38be3c...|SMALL_CITY_40|8b3a9c0c0df38be3c...|2020-11-02|   7|\n|2a6d49477833f2e9c...|SMALL_CITY_66|2a6d49477833f2e9c...|2020-11-02|  27|\n|06687142db4df0cd4...|SMALL_CITY_66|06687142db4df0cd4...|2020-11-02|   9|\n|edf5899963d271611...|SMALL_CITY_66|edf5899963d271611...|2020-11-02|  28|\n|259947e56017373ef...|SMALL_CITY_40|259947e56017373ef...|2020-11-02|  27|\n|49b4f78bc69459dcb...|SMALL_CITY_66|49b4f78bc69459dcb...|2020-11-02|  26|\n|cba4457b57f698ed5...|SMALL_CITY_40|cba4457b57f698ed5...|2020-11-02|  26|\n|31da9c29ded3a9f63...|SMALL_CITY_40|31da9c29ded3a9f63...|2020-11-02|  29|\n|fec7fcb414ccc868a...|SMALL_CITY_66|fec7fcb414ccc868a...|2020-11-02|  17|\n|c7b364ffc893ed39c...|SMALL_CITY_40|c7b364ffc893ed39c...|2020-11-02|   3|\n|2e0932b23b84ee617...|SMALL_CITY_40|2e0932b23b84ee617...|2020-11-02|  14|\n|ab79e55b745bbb3dc...|SMALL_CITY_66|ab79e55b745bbb3dc...|2020-11-02|  22|\n|b5c1b75d05e8c05df...|SMALL_CITY_40|b5c1b75d05e8c05df...|2020-11-02|   6|\n|3940d4a882cbb7d32...|SMALL_CITY_66|3940d4a882cbb7d32...|2020-11-02|  22|\n|cea0d0fbc86e6099c...|SMALL_CITY_66|cea0d0fbc86e6099c...|2020-11-02|  18|\n|4ed27964392364e0f...|SMALL_CITY_66|4ed27964392364e0f...|2020-11-02|   7|\n|b487bc47b8e92699e...|SMALL_CITY_40|b487bc47b8e92699e...|2020-11-02|  15|\n|9abaed31237f011a8...|SMALL_CITY_40|9abaed31237f011a8...|2020-11-02|   0|\n+--------------------+-------------+--------------------+----------+----+\nonly showing top 20 rows\n\n+--------------+----+-----+\n|city          |salt|count|\n+--------------+----+-----+\n|SMALL_CITY_0  |14  |121  |\n|SMALL_CITY_0  |12  |122  |\n|SMALL_CITY_100|27  |134  |\n|SMALL_CITY_0  |13  |137  |\n|SMALL_CITY_100|15  |138  |\n|SMALL_CITY_100|19  |139  |\n|SMALL_CITY_100|14  |140  |\n|SMALL_CITY_0  |11  |140  |\n|SMALL_CITY_100|12  |141  |\n|SMALL_CITY_0  |28  |142  |\n|SMALL_CITY_0  |22  |143  |\n|SMALL_CITY_100|26  |143  |\n|SMALL_CITY_100|11  |144  |\n|SMALL_CITY_100|17  |145  |\n|SMALL_CITY_0  |27  |145  |\n|SMALL_CITY_0  |21  |146  |\n|SMALL_CITY_0  |29  |146  |\n|SMALL_CITY_100|21  |146  |\n|SMALL_CITY_100|22  |147  |\n|SMALL_CITY_0  |24  |148  |\n+--------------+----+-----+\nonly showing top 20 rows\n\n+-------------+---------+\n|city         |final_sum|\n+-------------+---------+\n|BIG_CITY     |8998520  |\n|SMALL_CITY_45|10260    |\n|SMALL_CITY_15|10238    |\n|SMALL_CITY_6 |10213    |\n|SMALL_CITY_4 |10189    |\n|SMALL_CITY_23|10185    |\n|SMALL_CITY_13|10161    |\n|SMALL_CITY_90|10155    |\n|SMALL_CITY_79|10154    |\n|SMALL_CITY_87|10147    |\n|SMALL_CITY_74|10146    |\n|SMALL_CITY_58|10142    |\n|SMALL_CITY_14|10126    |\n|SMALL_CITY_69|10115    |\n|SMALL_CITY_60|10109    |\n|SMALL_CITY_42|10105    |\n|SMALL_CITY_70|10099    |\n|SMALL_CITY_19|10097    |\n|SMALL_CITY_8 |10094    |\n|SMALL_CITY_72|10091    |\n+-------------+---------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=3",
              "$$hashKey": "object:1336"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=4",
              "$$hashKey": "object:1337"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=5",
              "$$hashKey": "object:1338"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=6",
              "$$hashKey": "object:1339"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667918755088_343016848",
      "id": "paragraph_1667918755088_343016848",
      "dateCreated": "2022-11-08T17:45:55+0300",
      "dateStarted": "2022-11-08T20:05:02+0300",
      "dateFinished": "2022-11-08T20:05:28+0300",
      "status": "FINISHED",
      "$$hashKey": "object:109"
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.functions import col, lit, when\n\n#salted_agg_df\n#concat(col(“city_id”), lit(“_”), col(“salt”)\n#salted_agg_df.withColumn(\"city_salt\", f.concat(col(\"city\"), lit(\"_\"), col(\"salt\"))).show()\nsalted_agg_df2 = salted_agg_df.withColumn(\"city_salt\", when(col(\"city\").rlike(\"^BIG\"), f.concat(col(\"city\"), lit(\"_\"), col(\"salt\"))).otherwise(\"CITY\"))",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:05:32+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667924364033_354877543",
      "id": "paragraph_1667924364033_354877543",
      "dateCreated": "2022-11-08T19:19:24+0300",
      "dateStarted": "2022-11-08T20:05:32+0300",
      "dateFinished": "2022-11-08T20:05:32+0300",
      "status": "FINISHED",
      "$$hashKey": "object:110"
    },
    {
      "text": "%pyspark\n\nagg_df = salted_agg_df2 \\\n    .groupBy(f.col(\"city_salt\")).agg(f.sum(\"count\").alias(\"final_sum\")) \\\n    .orderBy(f.col(\"final_sum\").desc())\n    \nagg_df.show(40, False)  ",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:05:37+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+---------+\n|city_salt  |final_sum|\n+-----------+---------+\n|CITY       |1001480  |\n|BIG_CITY_6 |360716   |\n|BIG_CITY_4 |360516   |\n|BIG_CITY_3 |360372   |\n|BIG_CITY_1 |360171   |\n|BIG_CITY_2 |360097   |\n|BIG_CITY_9 |359751   |\n|BIG_CITY_7 |359595   |\n|BIG_CITY_8 |359479   |\n|BIG_CITY_5 |359043   |\n|BIG_CITY_0 |314438   |\n|BIG_CITY_10|314064   |\n|BIG_CITY_16|270600   |\n|BIG_CITY_15|270495   |\n|BIG_CITY_23|270431   |\n|BIG_CITY_21|270388   |\n|BIG_CITY_24|270381   |\n|BIG_CITY_26|270357   |\n|BIG_CITY_18|270244   |\n|BIG_CITY_27|270094   |\n|BIG_CITY_12|270019   |\n|BIG_CITY_20|269989   |\n|BIG_CITY_25|269964   |\n|BIG_CITY_11|269960   |\n|BIG_CITY_22|269911   |\n|BIG_CITY_19|269818   |\n|BIG_CITY_28|269767   |\n|BIG_CITY_14|269754   |\n|BIG_CITY_29|269735   |\n|BIG_CITY_13|269470   |\n|BIG_CITY_17|268901   |\n+-----------+---------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=7",
              "$$hashKey": "object:1366"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=8",
              "$$hashKey": "object:1367"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667925490793_1613865443",
      "id": "paragraph_1667925490793_1613865443",
      "dateCreated": "2022-11-08T19:38:10+0300",
      "dateStarted": "2022-11-08T20:05:37+0300",
      "dateFinished": "2022-11-08T20:05:54+0300",
      "status": "FINISHED",
      "$$hashKey": "object:111"
    },
    {
      "text": "%md\n\n#### Итого, получаем датафрейм с нормальным распределением значений городов:\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:15:52+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h4>Итого, получаем датафрейм с нормальным распределением значений городов:</h4>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667927572679_1000213024",
      "id": "paragraph_1667927572679_1000213024",
      "dateCreated": "2022-11-08T20:12:52+0300",
      "dateStarted": "2022-11-08T20:15:52+0300",
      "dateFinished": "2022-11-08T20:15:52+0300",
      "status": "FINISHED",
      "$$hashKey": "object:112"
    },
    {
      "text": "%pyspark\n\ndfs = salted_agg_df2.withColumn(\"s\", when(salted_agg_df2.city_salt == \"CITY\", salted_agg_df2.city).otherwise(salted_agg_df2.city_salt))\n\ndfs.groupBy(f.col(\"s\")).agg(f.sum(\"count\").alias(\"final_sum\")) \\\n    .orderBy(f.col(\"final_sum\").desc()).show()\n\n#df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n #                                .when(df.gender == \"F\",\"Female\")\n  #                               .when(df.gender.isNull() ,\"\")\n   #                              .otherwise(df.gender))",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T20:11:30+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+---------+\n|          s|final_sum|\n+-----------+---------+\n| BIG_CITY_6|   360716|\n| BIG_CITY_4|   360516|\n| BIG_CITY_3|   360372|\n| BIG_CITY_1|   360171|\n| BIG_CITY_2|   360097|\n| BIG_CITY_9|   359751|\n| BIG_CITY_7|   359595|\n| BIG_CITY_8|   359479|\n| BIG_CITY_5|   359043|\n| BIG_CITY_0|   314438|\n|BIG_CITY_10|   314064|\n|BIG_CITY_16|   270600|\n|BIG_CITY_15|   270495|\n|BIG_CITY_23|   270431|\n|BIG_CITY_21|   270388|\n|BIG_CITY_24|   270381|\n|BIG_CITY_26|   270357|\n|BIG_CITY_18|   270244|\n|BIG_CITY_27|   270094|\n|BIG_CITY_12|   270019|\n+-----------+---------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=18",
              "$$hashKey": "object:1390"
            },
            {
              "jobUrl": "http://ca-spark-zp-01.innoca.local:4040/jobs/job?id=19",
              "$$hashKey": "object:1391"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667925938281_1650056726",
      "id": "paragraph_1667925938281_1650056726",
      "dateCreated": "2022-11-08T19:45:38+0300",
      "dateStarted": "2022-11-08T20:11:30+0300",
      "dateFinished": "2022-11-08T20:11:46+0300",
      "status": "FINISHED",
      "$$hashKey": "object:113"
    },
    {
      "text": "%pyspark\n\n# пробовал через sql выражение, не получилось\n\nfrom pyspark.sql.functions import expr\n\nskew_df.withColumn(\"Name\",expr(\"CASE WHEN city = 'SMALL%' THEN 'CITY'\")).show(truncate=False)\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-08T19:49:12+0300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 5: skew_df.withColumn(\"Name\",expr(\"CASE WHEN city = 'SMALL%' THEN 'CITY'\")).show(truncate=False)\nTraceback (most recent call last):\n  File \"/tmp/1667922206756-0/zeppelin_python.py\", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 5, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 640, in expr\n    return Column(sc._jvm.functions.expr(str))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 73, in deco\n    raise ParseException(s.split(': ', 1)[1], stackTrace)\nParseException: u\"\\nmismatched input 'city' expecting <EOF>(line 1, pos 10)\\n\\n== SQL ==\\nCASE WHEN city = 'SMALL%' THEN 'CITY'\\n----------^^^\\n\"\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667922738915_191069407",
      "id": "paragraph_1667922738915_191069407",
      "dateCreated": "2022-11-08T18:52:18+0300",
      "dateStarted": "2022-11-08T19:02:17+0300",
      "dateFinished": "2022-11-08T19:02:17+0300",
      "status": "ERROR",
      "$$hashKey": "object:114"
    },
    {
      "text": "spark.stop",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_1841446736",
      "id": "20201128-174934_1428813475",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:115"
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2022-11-07T18:47:11+0300",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1667836031230_1408636498",
      "id": "20201128-175938_2140404686",
      "dateCreated": "2022-11-07T18:47:11+0300",
      "status": "READY",
      "$$hashKey": "object:116"
    }
  ],
  "name": "4",
  "id": "2HJU84FNF",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/user/skovin/homework/4"
}