{
  "metadata": {
    "name": "2",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# ДЗ 2. Срок -- 30 октября 18:00\n\nтетрадь не для редактирования -- перед выполнением сделать копию"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "* Скопировать тетрадку в тетрадку /user/\u003cлогин\u003e/homework/2\n(Копировать путь точно, с \"/\" в начале, кнопка скопировать пятая по счету слева в верхнем ряду)\nЕсли есть другие тетрадки, переместить их в ту же папку. Для перемещения можно прописать полный путь в заголовке\n\n* \u003cb\u003eПроверить, что в правах доступа (кнопка запка вверху справа) во всех полях свой логин и eakotelnikov\u003c/b\u003e\n\n* Дополнить параграфы своим кодом, чтобы получился аналогичный вывод параграфов\n\n* Прислать ссылку на решение преподавателю\n* \n\nДля освобождения ресурсов на кластере стоит лимит 30 минут на время выполнения приложения. Если SparkContext not found и приложение в ярне Killed, просто перезапустите интерпретер (кнопка шестеренки вверху справа)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pyspark.sql.functions as f\n\nevents_df \u003d spark.table(\"market.events\")\n\nevents_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pyspark.sql.functions as f\n\nevents_df \u003d spark.table(\"market.events\")\n\n\nevents_df \\\n    .filter(f.col(\"event_type\") \u003d\u003d \"view\") \\\n    .groupBy(\"category_code\") \\\n    .agg(f.count(\"*\").alias(\"view_count\")) \\\n    .orderBy(f.col(\u0027view_count\u0027).desc()) \\\n    .show(truncate\u003dFalse)\n    \n# category_id | views"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pyspark.sql.functions as f\n\nevents_df \u003d spark.table(\"market.events\")\n\nz.show(\n    events_df \\\n        .filter(f.col(\"date\") \u003d\u003d \"2019-11-17\") \\\n        .filter(f.col(\"event_type\") \u003d\u003d \"purchase\") \\\n        .withColumn(\"price_bin\", f.expr(\"round(price/10)*10\")) \\\n        .groupBy(\"price_bin\") \\\n        .agg(f.approx_count_distinct(\"product_id\")) \\\n        .orderBy(\"price_bin\")\n)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}